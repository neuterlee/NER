{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Revisar espacios en blanco\n",
    "\n",
    "### Descripción del Código:\n",
    "Este script analiza archivos JSON que contienen anotaciones para tareas de reconocimiento de entidades nombradas (NER). Su objetivo principal es verificar si las entidades tienen espacios en blanco inválidos al inicio o al final de los textos que abarcan. El script genera un resumen de los resultados y guarda un archivo de log con los detalles de las entidades problemáticas.\n",
    "\n",
    "### Cómo usar:\n",
    "1. Coloca los archivos JSON que deseas analizar en una carpeta llamada `input_annotations` en el mismo directorio donde se encuentra este script.\n",
    "2. Ejecuta el script.\n",
    "3. El resumen de los resultados se mostrará en la terminal.\n",
    "4. Un archivo de log detallado se guardará en la carpeta `logs` con el nombre `check_whitespace_log.txt`.\n",
    "\n",
    "### Resultado:\n",
    "- El script procesará todos los archivos JSON en la carpeta `input_annotations`.\n",
    "- Identificará entidades con espacios en blanco inválidos y las reportará en el log y en la terminal.\n",
    "- Si no se encuentran problemas, el log indicará que no hay espacios en blanco inválidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivos...\n",
      "\n",
      "=== Resumen ===\n",
      "Total de archivos procesados: 1\n",
      "Total de anotaciones: 51\n",
      "Total de entidades analizadas: 105\n",
      "Entidades con espacios en blanco inválidos: 0\n",
      "\n",
      "No se encontraron espacios en blanco inválidos en ninguna entidad.\n",
      "\n",
      "Archivo de log guardado en: /home/sotavento/Documents/tejer_red/1_procesamiento_anotaciones/logs/check_whitespace_log.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the directory containing annotation files (must be in the same directory as this script)\n",
    "annotations_directory = os.path.join(os.getcwd(), \"input_annotations\")\n",
    "\n",
    "# Define the logs directory\n",
    "logs_directory = os.path.join(os.getcwd(), \"logs\")\n",
    "os.makedirs(logs_directory, exist_ok=True)  # Create the logs directory if it doesn't exist\n",
    "\n",
    "# Define the log file name\n",
    "log_file_path = os.path.join(logs_directory, f\"check_whitespace_log.txt\")\n",
    "\n",
    "# Initialize a summary dictionary\n",
    "summary = {\n",
    "    \"total_files\": 0,\n",
    "    \"total_annotations\": 0,\n",
    "    \"total_entities\": 0,\n",
    "    \"invalid_whitespace\": []\n",
    "}\n",
    "\n",
    "# Check if the annotations directory exists\n",
    "if not os.path.exists(annotations_directory):\n",
    "    print(f\"The folder 'input_annotations' does not exist in the current directory: {os.getcwd()}\")\n",
    "    exit()\n",
    "\n",
    "# Process all JSON files in the directory\n",
    "files = [f for f in os.listdir(annotations_directory) if f.endswith(\".json\")]\n",
    "if not files:\n",
    "    print(f\"No JSON files found in the directory: {annotations_directory}\")\n",
    "    exit()\n",
    "\n",
    "# Open the log file for writing\n",
    "with open(log_file_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "    log_file.write(\"=== NER Annotation Analysis Log ===\\n\")\n",
    "    log_file.write(f\"Processed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Directory: {annotations_directory}\\n\\n\")\n",
    "\n",
    "    print(\"Processing files...\\n\")\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(annotations_directory, file_name)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {file_path}\")\n",
    "            log_file.write(f\"File not found: {file_path}\\n\")\n",
    "            continue\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON file: {file_path}\")\n",
    "            log_file.write(f\"Error decoding JSON file: {file_path}\\n\")\n",
    "            continue\n",
    "\n",
    "        # Update summary\n",
    "        annotations = data.get(\"annotations\", [])\n",
    "        summary[\"total_files\"] += 1\n",
    "        summary[\"total_annotations\"] += len(annotations)\n",
    "\n",
    "        # Check for invalid whitespace in entity spans\n",
    "        for i, (text, annotation) in enumerate(annotations):\n",
    "            for entity in annotation[\"entities\"]:\n",
    "                start, end, label = entity\n",
    "                span_text = text[start:end]\n",
    "                summary[\"total_entities\"] += 1\n",
    "                if span_text != span_text.strip():\n",
    "                    summary[\"invalid_whitespace\"].append({\n",
    "                        \"file_name\": file_name,\n",
    "                        \"annotation_index\": i,\n",
    "                        \"entity_text\": span_text,\n",
    "                        \"label\": label,\n",
    "                        \"start\": start,\n",
    "                        \"end\": end\n",
    "                    })\n",
    "\n",
    "    # Write summary to the log file\n",
    "    log_file.write(\"=== Summary ===\\n\")\n",
    "    log_file.write(f\"Total files processed: {summary['total_files']}\\n\")\n",
    "    log_file.write(f\"Total annotations: {summary['total_annotations']}\\n\")\n",
    "    log_file.write(f\"Total entities analyzed: {summary['total_entities']}\\n\")\n",
    "    log_file.write(f\"Entities with invalid whitespace: {len(summary['invalid_whitespace'])}\\n\\n\")\n",
    "\n",
    "    if summary[\"invalid_whitespace\"]:\n",
    "        log_file.write(\"=== Invalid Whitespace Entities ===\\n\")\n",
    "        for issue in summary[\"invalid_whitespace\"]:\n",
    "            log_file.write(f\"- File: {issue['file_name']}, Annotation {issue['annotation_index']}: \"\n",
    "                           f\"'{issue['entity_text']}' (label: {issue['label']}, \"\n",
    "                           f\"start: {issue['start']}, end: {issue['end']})\\n\")\n",
    "    else:\n",
    "        log_file.write(\"No invalid whitespace found in any entity.\\n\")\n",
    "\n",
    "# Print summary to the terminal\n",
    "print(\"=== Summary ===\")\n",
    "print(f\"Total files processed: {summary['total_files']}\")\n",
    "print(f\"Total annotations: {summary['total_annotations']}\")\n",
    "print(f\"Total entities analyzed: {summary['total_entities']}\")\n",
    "print(f\"Entities with invalid whitespace: {len(summary['invalid_whitespace'])}\\n\")\n",
    "\n",
    "if summary[\"invalid_whitespace\"]:\n",
    "    print(\"=== Invalid Whitespace Entities ===\")\n",
    "    for issue in summary[\"invalid_whitespace\"]:\n",
    "        print(f\"- File: {issue['file_name']}, Annotation {issue['annotation_index']}: \"\n",
    "              f\"'{issue['entity_text']}' (label: {issue['label']}, \"\n",
    "              f\"start: {issue['start']}, end: {issue['end']})\")\n",
    "else:\n",
    "    print(\"No invalid whitespace found in any entity.\")\n",
    "\n",
    "print(f\"\\nLog file saved to: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Combinar anotaciones\n",
    "\n",
    "### Descripción del Código:\n",
    "Este script combina múltiples archivos JSON que contienen anotaciones para tareas de reconocimiento de entidades nombradas (NER). Su objetivo es unificar las clases y anotaciones de todos los archivos en un único archivo llamado `combined_annotations.json`. Además, genera un archivo de log con un resumen del proceso.\n",
    "\n",
    "### Cómo usar:\n",
    "1. Coloca los archivos JSON que deseas combinar en una carpeta llamada `input_annotations` en el mismo directorio donde se encuentra este script.\n",
    "2. Ejecuta el script.\n",
    "3. El archivo combinado se guardará en el mismo directorio donde se ejecuta el script con el nombre `combined_annotations.json`.\n",
    "4. Un archivo de log con el resumen del proceso se guardará en la carpeta `logs` con el nombre `merge_log.txt`.\n",
    "\n",
    "### Resultado:\n",
    "- El script procesará todos los archivos JSON en la carpeta `input_annotations`.\n",
    "- Unificará las clases y anotaciones, evitando duplicados.\n",
    "- Guardará el archivo combinado en el directorio actual y generará un log con los detalles del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined file saved as: /home/sotavento/Documents/tejer_red/1_procesamiento_anotaciones/merged_annotations.json\n",
      "Log file saved as: /home/sotavento/Documents/tejer_red/1_procesamiento_anotaciones/logs/merge_log.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the input and logs directories\n",
    "annotations_directory = os.path.join(os.getcwd(), \"input_annotations\")\n",
    "logs_directory = os.path.join(os.getcwd(), \"logs\")\n",
    "\n",
    "# Ensure the logs directory exists\n",
    "os.makedirs(logs_directory, exist_ok=True)\n",
    "\n",
    "# Generate the output file name with the current date and time\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_file = os.path.join(os.getcwd(), f\"merged_annotations.json\")\n",
    "\n",
    "# Generate the log file name\n",
    "script_name = \"merge\"  # Use a hardcoded name for the script\n",
    "log_file_path = os.path.join(logs_directory, f\"merge_log.txt\")\n",
    "\n",
    "# Initialize lists for classes and annotations\n",
    "classes = []\n",
    "annotations = []\n",
    "\n",
    "# Check if the input directory exists\n",
    "if not os.path.exists(annotations_directory):\n",
    "    print(f\"The folder 'input_annotations' does not exist in the current directory: {os.getcwd()}\")\n",
    "    exit()\n",
    "\n",
    "# Get all JSON files in the input directory\n",
    "files = [os.path.join(annotations_directory, f) for f in os.listdir(annotations_directory) if f.endswith(\".json\")]\n",
    "\n",
    "if not files:\n",
    "    print(f\"No JSON files found in the directory: {annotations_directory}\")\n",
    "    exit()\n",
    "\n",
    "# Open the log file for writing\n",
    "with open(log_file_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "    log_file.write(\"=== JSON Merge Log ===\\n\")\n",
    "    log_file.write(f\"Script executed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    log_file.write(f\"Input directory: {annotations_directory}\\n\")\n",
    "    log_file.write(f\"Output file: {output_file}\\n\\n\")\n",
    "\n",
    "    # Process each JSON file\n",
    "    log_file.write(\"Processing files:\\n\")\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(file, encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                log_file.write(f\"- Successfully loaded: {file}\\n\")\n",
    "                # Add classes while avoiding duplicates\n",
    "                for cls in data.get(\"classes\", []):\n",
    "                    if cls not in classes:\n",
    "                        classes.append(cls)\n",
    "                # Add annotations while avoiding duplicates and filtering those with entities\n",
    "                for ann in data.get(\"annotations\", []):\n",
    "                    if ann not in annotations and ann[1].get(\"entities\"):  # Only add if there are entities\n",
    "                        annotations.append(ann)\n",
    "        except json.JSONDecodeError:\n",
    "            log_file.write(f\"- Error reading JSON file: {file}\\n\")\n",
    "            print(f\"Error reading JSON file: {file}\")\n",
    "            continue\n",
    "\n",
    "    # Create the combined JSON structure\n",
    "    combined_data = {\n",
    "        \"classes\": classes,\n",
    "        \"annotations\": annotations\n",
    "    }\n",
    "\n",
    "    # Save the combined file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined_data, f, ensure_ascii=False, indent=4)  # Use indentation for better readability\n",
    "\n",
    "    log_file.write(\"\\n=== Summary ===\\n\")\n",
    "    log_file.write(f\"Total files processed: {len(files)}\\n\")\n",
    "    log_file.write(f\"Total unique classes: {len(classes)}\\n\")\n",
    "    log_file.write(f\"Total annotations: {len(annotations)}\\n\")\n",
    "    log_file.write(f\"Combined file saved as: {output_file}\\n\")\n",
    "\n",
    "print(f\"Combined file saved as: {output_file}\")\n",
    "print(f\"Log file saved as: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Eliminacion de entidades inecesarias (opcional)\n",
    "\n",
    "### Descripción del Código:\n",
    "Este script permite eliminar entidades específicas de un archivo JSON llamado `merged_annotations.json`, que contiene anotaciones para tareas de reconocimiento de entidades nombradas (NER). El usuario puede seleccionar las entidades a eliminar mediante un rango o números individuales. El archivo actualizado se guarda como `purged_annotations.json` en el mismo directorio, y se genera un archivo de log con los detalles del proceso en la carpeta `logs`.\n",
    "\n",
    "### Cómo usar:\n",
    "1. Asegúrate de que el archivo `merged_annotations.json` esté en el mismo directorio donde se encuentra este script.\n",
    "2. Ejecuta el script.\n",
    "3. El script mostrará todas las entidades disponibles y te pedirá que selecciones cuáles deseas eliminar (usando números o rangos, por ejemplo, `1-3,5`).\n",
    "4. Confirma la selección para proceder con la eliminación.\n",
    "5. El archivo actualizado se guardará como `purged_annotations.json` en el mismo directorio.\n",
    "6. Un archivo de log con los detalles del proceso se guardará en la carpeta `logs` con el nombre `purge_log.txt`.\n",
    "\n",
    "### Resultado:\n",
    "- El archivo `purged_annotations.json` contendrá las anotaciones actualizadas sin las entidades seleccionadas.\n",
    "- El log incluirá un resumen de las entidades eliminadas y las que permanecen en el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations from: /home/sotavento/Documents/tejer_red/1_procesamiento_anotaciones/merged_annotations.json\n",
      "\n",
      "Available entities in the annotations:\n",
      "1. NOMBRE\n",
      "2. DOMICILIO\n",
      "3. FECHA\n",
      "4. HORA\n",
      "5. TELEFONO\n",
      "6. EXP\n",
      "7. PLACA\n",
      "\n",
      "Enter the numbers or ranges of the entities you want to remove, separated by commas (e.g., 1-5,7,10-12):\n",
      "\n",
      "You have selected the following entities to remove:\n",
      "- HORA\n",
      "\n",
      "Purging selected entities...\n",
      "\n",
      "Entities purged successfully. Updated file saved to: /home/sotavento/Documents/tejer_red/1_procesamiento_anotaciones/purged_annotations.json\n",
      "\n",
      "Summary:\n",
      "Entities removed: HORA\n",
      "Remaining entities: NOMBRE, DOMICILIO, FECHA, TELEFONO, EXP, PLACA\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load the JSON file containing annotations.\n",
    "    :param file_path: Path to the JSON file.\n",
    "    :return: Parsed JSON data.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save the JSON data to a file.\n",
    "    :param data: JSON data to save.\n",
    "    :param file_path: Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "def purge_entities(data, entities_to_remove):\n",
    "    \"\"\"\n",
    "    Remove specified entities from the JSON data.\n",
    "    :param data: Original JSON data.\n",
    "    :param entities_to_remove: List of entities to remove.\n",
    "    :return: Updated JSON data.\n",
    "    \"\"\"\n",
    "    # Remove entities from the \"classes\" list\n",
    "    data[\"classes\"] = [entity for entity in data[\"classes\"] if entity not in entities_to_remove]\n",
    "\n",
    "    # Remove entities from the \"annotations\"\n",
    "    for annotation in data[\"annotations\"]:\n",
    "        annotation[1][\"entities\"] = [\n",
    "            entity for entity in annotation[1][\"entities\"] if entity[2] not in entities_to_remove\n",
    "        ]\n",
    "    return data\n",
    "\n",
    "def parse_selection(selection, total_entities):\n",
    "    \"\"\"\n",
    "    Parse the user's selection of entities, allowing ranges and individual numbers.\n",
    "    :param selection: String input from the user (e.g., \"1-5,7,10-12\").\n",
    "    :param total_entities: Total number of entities available.\n",
    "    :return: List of selected indices.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    try:\n",
    "        parts = selection.split(\",\")\n",
    "        for part in parts:\n",
    "            if \"-\" in part:  # Handle ranges (e.g., \"1-5\")\n",
    "                start, end = map(int, part.split(\"-\"))\n",
    "                indices.extend(range(start, end + 1))\n",
    "            else:  # Handle individual numbers (e.g., \"7\")\n",
    "                indices.append(int(part))\n",
    "        # Ensure indices are within valid range\n",
    "        indices = [idx for idx in indices if 1 <= idx <= total_entities]\n",
    "        return sorted(set(indices))  # Remove duplicates and sort\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Invalid selection format. Use numbers or ranges (e.g., 1-5,7,10-12).\")\n",
    "\n",
    "def main():\n",
    "    # Define the input file and output folder\n",
    "    input_file = os.path.join(os.getcwd(), \"merged_annotations.json\")\n",
    "    output_folder = os.getcwd()  # Save output in the same folder as the script\n",
    "    logs_folder = os.path.join(os.getcwd(), \"logs\")\n",
    "    os.makedirs(logs_folder, exist_ok=True)  # Ensure the logs folder exists\n",
    "\n",
    "    # Generate the log file name\n",
    "    log_file_path = os.path.join(logs_folder, f\"purge_log.txt\")\n",
    "\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"The file 'combined_annotations.json' does not exist in the current directory: {os.getcwd()}\")\n",
    "        return\n",
    "\n",
    "    # Generate the output file name with a timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = os.path.join(output_folder, f\"purged_annotations.json\")\n",
    "\n",
    "    # Open the log file for writing\n",
    "    with open(log_file_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "        log_file.write(\"=== Purging Log ===\\n\")\n",
    "        log_file.write(f\"Script executed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        log_file.write(f\"Input file: {input_file}\\n\")\n",
    "        log_file.write(f\"Output file: {output_file}\\n\\n\")\n",
    "\n",
    "        # Load the JSON file\n",
    "        log_file.write(f\"Loading annotations from: {input_file}\\n\")\n",
    "        print(f\"Loading annotations from: {input_file}\")\n",
    "        data = load_json(input_file)\n",
    "\n",
    "        # List all unique entities from \"classes\"\n",
    "        log_file.write(\"\\nAvailable entities in the annotations:\\n\")\n",
    "        print(\"\\nAvailable entities in the annotations:\")\n",
    "        all_entities = data[\"classes\"]\n",
    "        for i, entity in enumerate(all_entities, start=1):\n",
    "            log_file.write(f\"{i}. {entity}\\n\")\n",
    "            print(f\"{i}. {entity}\")\n",
    "\n",
    "        # Ask the user which entities to remove\n",
    "        print(\"\\nEnter the numbers or ranges of the entities you want to remove, separated by commas (e.g., 1-5,7,10-12):\")\n",
    "        selection = input(\"Your selection: \").strip()\n",
    "        try:\n",
    "            selected_indices = parse_selection(selection, len(all_entities))\n",
    "            entities_to_remove = [all_entities[idx - 1] for idx in selected_indices]\n",
    "        except ValueError as e:\n",
    "            log_file.write(f\"Error: {e}\\n\")\n",
    "            print(f\"Error: {e}\")\n",
    "            return\n",
    "\n",
    "        # Confirm the selection\n",
    "        log_file.write(\"\\nSelected entities to remove:\\n\")\n",
    "        print(\"\\nYou have selected the following entities to remove:\")\n",
    "        for entity in entities_to_remove:\n",
    "            log_file.write(f\"- {entity}\\n\")\n",
    "            print(f\"- {entity}\")\n",
    "        confirm = input(\"\\nDo you want to proceed? (yes/no): \").strip().lower()\n",
    "        if confirm != \"yes\":\n",
    "            log_file.write(\"Operation cancelled by the user.\\n\")\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "\n",
    "        # Purge the selected entities\n",
    "        log_file.write(\"\\nPurging selected entities...\\n\")\n",
    "        print(\"\\nPurging selected entities...\")\n",
    "        updated_data = purge_entities(data, entities_to_remove)\n",
    "\n",
    "        # Save the updated JSON file\n",
    "        save_json(updated_data, output_file)\n",
    "        log_file.write(f\"\\nEntities purged successfully. Updated file saved to: {output_file}\\n\")\n",
    "        print(f\"\\nEntities purged successfully. Updated file saved to: {output_file}\")\n",
    "\n",
    "        # Summary\n",
    "        log_file.write(\"\\nSummary:\\n\")\n",
    "        log_file.write(f\"Entities removed: {', '.join(entities_to_remove)}\\n\")\n",
    "        log_file.write(f\"Remaining entities: {', '.join([e for e in all_entities if e not in entities_to_remove])}\\n\")\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Entities removed: {', '.join(entities_to_remove)}\")\n",
    "        print(f\"Remaining entities: {', '.join([e for e in all_entities if e not in entities_to_remove])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizar anotaciones después de combinar, depurar y antes del entrenamiento\n",
    "\n",
    "### Descripción del Código:\n",
    "Este script genera un archivo HTML interactivo para visualizar anotaciones de reconocimiento de entidades nombradas (NER) a partir de un archivo JSON llamado `purged_annotations.json`. Las entidades detectadas se resaltan en el texto con colores específicos, y se genera un archivo de log con los detalles del proceso.\n",
    "\n",
    "### Cómo usar:\n",
    "1. Asegúrate de que el archivo `purged_annotations.json` esté en el mismo directorio donde se encuentra este script.\n",
    "2. Ejecuta el script.\n",
    "3. El archivo HTML generado se guardará en el mismo directorio con el nombre `annotations_visualizer.html`.\n",
    "4. Un archivo de log con los detalles del proceso se guardará en la carpeta `logs` con el nombre `visualizer_log.txt`.\n",
    "\n",
    "### Resultado:\n",
    "- El archivo HTML mostrará las anotaciones resaltadas con colores, indicando las entidades y sus etiquetas.\n",
    "- El archivo de log incluirá un resumen de las entidades detectadas y los colores asignados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"\n",
    "    Load the JSON file containing annotations.\n",
    "    :param file_path: Path to the JSON file.\n",
    "    :return: Parsed JSON data.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def save_file(content, file_path):\n",
    "    \"\"\"\n",
    "    Save content to a file.\n",
    "    :param content: Content to save.\n",
    "    :param file_path: Path to the output file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "def main():\n",
    "    # Define the input file and output file\n",
    "    input_file = os.path.join(os.getcwd(), \"purged_annotations.json\")\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = os.path.join(os.getcwd(), f\"annotations_visualizer.html\")\n",
    "    logs_folder = os.path.join(os.getcwd(), \"logs\")\n",
    "    os.makedirs(logs_folder, exist_ok=True)  # Ensure the logs folder exists\n",
    "    log_file = os.path.join(logs_folder, f\"visualizer_log.txt\")\n",
    "\n",
    "    # Check if the input file exists\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"The file 'purged_annotations.json' does not exist in the current directory: {os.getcwd()}\")\n",
    "        return\n",
    "\n",
    "    # Load the JSON file\n",
    "    print(f\"Loading annotations from: {input_file}\")\n",
    "    data = load_json(input_file)\n",
    "\n",
    "    # Dynamically detect entities from the file\n",
    "    detected_entities = set()\n",
    "    for annotation in data.get(\"annotations\", []):\n",
    "        for entity in annotation[1].get(\"entities\", []):\n",
    "            if len(entity) >= 3:\n",
    "                detected_entities.add(entity[2])\n",
    "\n",
    "    # Assign colors to detected entities\n",
    "    entity_colors = {}\n",
    "    color_palette = [\n",
    "        \"#FFCCCC\", \"#CCE5FF\", \"#FFFFCC\", \"#D5CCFF\", \"#FFCCF2\", \"#CCFFCC\",\n",
    "        \"#FFCC99\", \"#FF99CC\", \"#99CCFF\", \"#FF9966\", \"#99FF99\", \"#FF9999\",\n",
    "        \"#CCCCFF\", \"#99FFFF\"\n",
    "    ]\n",
    "    for i, entity in enumerate(sorted(detected_entities)):\n",
    "        entity_colors[entity] = color_palette[i % len(color_palette)]\n",
    "\n",
    "    # Generate HTML content\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>NER Annotations Visualizer</title>\n",
    "        <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; line-height: 1.6; padding: 20px; }\n",
    "            .entity { display: inline-block; padding: 2px 4px; margin: 1px; border-radius: 4px; font-size: 0.9em; color: #000; }\n",
    "            .entity small { font-size: 0.75em; color: #555; }\n",
    "            .annotation-container { margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; background-color: #f9f9f9; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            <h1 class=\"text-center my-4\">NER Annotations Visualizer</h1>\n",
    "            <p class=\"text-muted text-center\">This page highlights named entities in the text with their corresponding labels.</p>\n",
    "    \"\"\"\n",
    "\n",
    "    # Process each annotation\n",
    "    for annotation in data.get(\"annotations\", []):\n",
    "        if not annotation or not isinstance(annotation, list) or len(annotation) < 2:\n",
    "            continue\n",
    "\n",
    "        text = annotation[0]\n",
    "        entities = annotation[1].get(\"entities\", [])\n",
    "\n",
    "        # Sort entities by their start index to avoid overlapping issues\n",
    "        entities = sorted(entities, key=lambda x: x[0])\n",
    "\n",
    "        # Annotate the text\n",
    "        annotated_text = \"\"\n",
    "        last_index = 0\n",
    "        for entity in entities:\n",
    "            if len(entity) < 3:\n",
    "                continue\n",
    "            start, end, label = entity\n",
    "            # Add text before the entity\n",
    "            annotated_text += text[last_index:start]\n",
    "            # Add the entity with a span and color\n",
    "            color = entity_colors.get(label, \"#E0E0E0\")  # Default color if label not found\n",
    "            annotated_text += f'<span class=\"entity\" style=\"background-color: {color};\">{text[start:end]} <small>({label})</small></span>'\n",
    "            last_index = end\n",
    "        # Add remaining text after the last entity\n",
    "        annotated_text += text[last_index:]\n",
    "\n",
    "        # Add the annotated text to the HTML\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"annotation-container\">\n",
    "            <p>{annotated_text}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    # Close the HTML content\n",
    "    html_content += \"\"\"\n",
    "        </div>\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha3/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    # Save the HTML file\n",
    "    save_file(html_content, output_file)\n",
    "\n",
    "    # Write log file\n",
    "    with open(log_file, \"w\", encoding=\"utf-8\") as log:\n",
    "        log.write(\"=== NER Annotations Visualizer Log ===\\n\")\n",
    "        log.write(f\"Script executed on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        log.write(f\"Input file: {input_file}\\n\")\n",
    "        log.write(f\"Output HTML file: {output_file}\\n\")\n",
    "        log.write(\"\\nDetected Entities:\\n\")\n",
    "        for entity, color in entity_colors.items():\n",
    "            log.write(f\"- {entity}: {color}\\n\")\n",
    "\n",
    "    print(f\"HTML visualizer created: {output_file}\")\n",
    "    print(f\"Log file created: {log_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
